{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistortedText.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartika-nair/CAPTCHA-Solver/blob/master/DistortedText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcumiKKL7f9N"
      },
      "source": [
        "Distorted Text CAPTCHA solver - using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCPt2IrzfN3D"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import string\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import multiprocessing as mp\n",
        "\n",
        "import cv2\n",
        "import imutils\n",
        "import tqdm as tq\n",
        "import pickle\n",
        "import os.path\n",
        "from imutils import paths\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.core import Flatten, Dense\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2oH5CV5cvrK",
        "outputId": "38fa8029-c64f-4294-cbaa-fb9e30e8f554"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKKH7J6mEQmC"
      },
      "source": [
        "**PROJECT DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60Aan6iuQcZL"
      },
      "source": [
        "cpu_count = mp.cpu_count()\n",
        "\n",
        "IMAGES_FOLDER = \"/content/drive/MyDrive/Colab Notebooks/Images\"\n",
        "\n",
        "EXTRACT_IMAGES = 0 #SET THIS TO TRUE TO GENERATE IMAGES READBLE BY CODE\n",
        "NUMBER_OF_LETTERS = 4\n",
        "\n",
        "EXTRACTED_FOLDER = \"/content/drive/MyDrive/Colab Notebooks/EXTRACTED\"\n",
        "\n",
        "IMAGE_SIZE = 32"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYY3_1PSETrJ"
      },
      "source": [
        "**EXTRACT IMAGES FROM CAPTCHA IMAGES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOhWxyOFQ-uy"
      },
      "source": [
        "def extract_image(path = IMAGES_FOLDER, output = EXTRACTED_FOLDER):\n",
        "  captcha_image_files = glob.glob(os.path.join(path, \"*\"))\n",
        "  counts = {}\n",
        "  for (i, captcha_image_file) in tq.tqdm(enumerate(captcha_image_files)):\n",
        "    # print(f\"[DEBUG]{i}: {os.path.basename(captcha_image_file)}\")\n",
        "\n",
        "    try:\n",
        "      filename = os.path.basename(captcha_image_file)\n",
        "      captcha_correct_text = os.path.splitext(filename)[0]\n",
        "\n",
        "      image = cv2.imread(captcha_image_file)\n",
        "      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "      gray = cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
        "\n",
        "      thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "      contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "      contours = contours[1] if imutils.is_cv3() else contours[0]\n",
        "\n",
        "      letter_image_regions = []\n",
        "\n",
        "      for contour in contours:\n",
        "          (x, y, w, h) = cv2.boundingRect(contour)\n",
        "\n",
        "          if w / h > 1.25:\n",
        "\n",
        "              half_width = int(w / 2)\n",
        "              letter_image_regions.append((x, y, half_width, h))\n",
        "              letter_image_regions.append((x + half_width, y, half_width, h))\n",
        "\n",
        "          else:\n",
        "\n",
        "              letter_image_regions.append((x, y, w, h))\n",
        "\n",
        "      if len(letter_image_regions) != NUMBER_OF_LETTERS:\n",
        "          continue\n",
        "\n",
        "      letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])\n",
        "\n",
        "      for letter_bounding_box, letter_text in zip(letter_image_regions, captcha_correct_text):\n",
        "          x, y, w, h = letter_bounding_box\n",
        "          letter_image = gray[y - 2:y + h + 2, x - 2:x + w + 2]\n",
        "\n",
        "          save_path = os.path.join(output, \"\")\n",
        "\n",
        "      if not os.path.exists(save_path):\n",
        "          os.makedirs(save_path)\n",
        "\n",
        "      count = counts.get(letter_text, 1)\n",
        "      p = os.path.join(save_path, \"{}_{}.png\".format(letter_text, str(count).zfill(6)))\n",
        "      cv2.imwrite(p, letter_image)\n",
        "\n",
        "      counts[letter_text] = count + 1\n",
        "    except Exception as e:\n",
        "      pass"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljsgP8rVzPfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8130eb65-33d0-4299-f570-b86a8d643078"
      },
      "source": [
        "if EXTRACT_IMAGES:\n",
        "  extract_image()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9955it [1:05:53,  2.52it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQNbzGYMzZC3"
      },
      "source": [
        "def resize_to_fit(image, width = IMAGE_SIZE, height = IMAGE_SIZE):\n",
        "\n",
        "  (h, w) = image.shape[:2]\n",
        "\n",
        "  if w > h:\n",
        "    image = imutils.resize(image, width=width)\n",
        "  else:\n",
        "    image = imutils.resize(image, height=height)\n",
        "\n",
        "  padW = int((width - image.shape[1]) / 2.0)\n",
        "  padH = int((height - image.shape[0]) / 2.0)\n",
        "\n",
        "  image = cv2.copyMakeBorder(image, padH, padH, padW, padW, cv2.BORDER_REPLICATE)\n",
        "  image = cv2.resize(image, (width, height))\n",
        "\n",
        "  return image"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YXsK8G5EY66"
      },
      "source": [
        "**CUSTOM DATASET CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoOijxPDzolD"
      },
      "source": [
        "class CAPTCHADataset(Dataset):\n",
        "\n",
        "  def __init__(self, images, data_dir = EXTRACTED_FOLDER):\n",
        "    self.data_dir = data_dir\n",
        "    self.images = images\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    image_fn = self.images[index]\n",
        "    image = cv2.imread(f\"{self.data_dir}/{image_fn}\")\n",
        "    # image = Image.open(image_fp).convert('RGB')\n",
        "    image = resize_to_fit(image)\n",
        "    image = self.transform(image)\n",
        "    text = image_fn.split(\"_\")[0]\n",
        "    return image, text\n",
        "  \n",
        "  def transform(self, image):\n",
        "    transform_ops = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "    return transform_ops(image)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pNofuicEeMU"
      },
      "source": [
        "**ENCODE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYCjwAM_BjEX"
      },
      "source": [
        "images = os.listdir(EXTRACTED_FOLDER)\n",
        "image_fns_train, image_fns_test = train_test_split(images, random_state=0)\n",
        "image_ns = [image.split(\"_\")[0] for image in images]\n",
        "image_ns = \"\".join(image_ns)\n",
        "letters = sorted(list(set(list(image_ns))))\n",
        "vocabulary = [\"-\"] + letters\n",
        "idx2char = {k:v for k,v in enumerate(vocabulary, start=0)}\n",
        "char2idx = {v:k for k,v in idx2char.items()}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZpHnJP_GdcJ"
      },
      "source": [
        "**MODEL PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czE0ieDGGTU-"
      },
      "source": [
        "batch_size = 6\n",
        "num_epochs = 50\n",
        "lr = 0.001\n",
        "log_interval = 100\n",
        "gamma = 0.7\n",
        "num_chars = len(char2idx)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsQzFjj6GhzU"
      },
      "source": [
        "**DEFINE TEST AND TRAIN LOADER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdUrebylE3-w"
      },
      "source": [
        "trainset = CAPTCHADataset(images = image_fns_train, data_dir =  EXTRACTED_FOLDER) \n",
        "testset = CAPTCHADataset(images = image_fns_test, data_dir = EXTRACTED_FOLDER)\n",
        "train_loader = DataLoader(trainset, batch_size=batch_size, num_workers=cpu_count, shuffle=True)\n",
        "test_loader = DataLoader(testset, batch_size=batch_size, num_workers=cpu_count, shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGkFvwxiGa0I"
      },
      "source": [
        "image_batch, text_batch = iter(train_loader).next()\n",
        "num_chars = len(char2idx)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spLJ48NoZbOL"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, num_chars = num_chars):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "    self.dropout1 = nn.Dropout(0.25)\n",
        "    self.dropout2 = nn.Dropout(0.5)\n",
        "    self.fc1 = nn.Linear(12544, 128)\n",
        "    self.fc2 = nn.Linear(128, num_chars)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "    return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-hhZWz9iXlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d7215a-fea1-437c-ce4a-455693ac6090"
      },
      "source": [
        "modelTest = Net()\n",
        "modelTest(image_batch)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.7295, -3.7067, -3.5759, -3.8884, -3.7532, -3.7660, -3.8893, -3.9657,\n",
              "         -3.8400, -3.6846, -3.8432, -3.8274, -3.7876, -3.7650, -4.0837, -4.1099,\n",
              "         -3.6069, -3.8272, -3.8621, -3.6646, -4.0093, -3.7186, -4.1738, -4.0952,\n",
              "         -3.7059, -3.8672, -4.0765, -3.8492, -3.7806, -3.9071, -3.8525, -3.8461,\n",
              "         -3.7133, -3.5364, -3.7088, -3.9179, -3.8034, -3.7278, -3.9314, -3.8192,\n",
              "         -3.7876, -3.6111, -3.7608, -3.5669, -3.8232],\n",
              "        [-3.9640, -3.8047, -3.5731, -3.7277, -3.7980, -3.6339, -3.8768, -3.7852,\n",
              "         -3.7918, -3.8851, -3.8808, -3.8785, -3.6579, -3.5244, -3.8954, -3.9758,\n",
              "         -3.7337, -3.7925, -3.7934, -3.6685, -3.9604, -3.9075, -3.8709, -3.9319,\n",
              "         -3.8031, -3.9915, -4.0029, -4.1132, -3.7749, -3.9844, -3.8587, -3.8530,\n",
              "         -3.8359, -3.8541, -3.6955, -3.9491, -3.6976, -3.7705, -3.9908, -3.6807,\n",
              "         -3.4974, -3.6970, -3.9059, -3.6375, -3.7998],\n",
              "        [-3.9097, -3.6195, -3.7800, -3.9016, -3.9628, -3.7504, -3.8237, -4.1111,\n",
              "         -3.6732, -3.5487, -3.6793, -3.7929, -3.7943, -3.8135, -3.8909, -4.0758,\n",
              "         -3.9447, -4.0712, -3.9296, -3.8035, -3.8848, -3.9739, -3.7440, -3.5813,\n",
              "         -3.6546, -3.9121, -3.9680, -3.8786, -3.7042, -3.8888, -3.4875, -3.9111,\n",
              "         -4.1572, -3.6855, -4.0690, -4.0854, -3.6851, -3.8185, -3.6260, -3.9729,\n",
              "         -3.8672, -3.6345, -3.7520, -3.4860, -3.6286],\n",
              "        [-3.7041, -3.6904, -3.6626, -3.9425, -3.6361, -3.9281, -3.8985, -3.8934,\n",
              "         -3.7082, -3.5735, -4.1837, -3.7175, -3.8009, -3.5902, -3.6790, -4.0924,\n",
              "         -3.7738, -3.8863, -3.9668, -3.6275, -3.6501, -4.0520, -3.8511, -3.9198,\n",
              "         -3.6468, -3.9459, -4.0522, -3.6276, -3.7002, -3.9080, -4.0481, -3.9959,\n",
              "         -3.9162, -3.8822, -3.5701, -3.8112, -3.6836, -3.7443, -3.7599, -3.7959,\n",
              "         -3.7006, -3.6809, -4.1909, -3.7008, -4.1292],\n",
              "        [-3.8168, -3.6485, -3.8715, -3.8321, -3.5794, -3.9532, -3.5217, -3.7370,\n",
              "         -3.7699, -3.9562, -3.8182, -3.9123, -3.7542, -3.7691, -3.9148, -4.0428,\n",
              "         -3.8572, -3.9892, -3.8417, -3.8220, -3.9636, -3.7465, -4.1126, -3.9821,\n",
              "         -3.8179, -3.9767, -3.8775, -3.7501, -3.8175, -4.0216, -3.6958, -3.9721,\n",
              "         -3.8178, -3.6101, -3.8440, -3.9338, -3.6902, -3.8301, -3.6965, -3.6534,\n",
              "         -3.8024, -3.6690, -3.6586, -3.7548, -3.5969],\n",
              "        [-3.8141, -3.5243, -3.8664, -4.0021, -3.6327, -3.7285, -3.8599, -3.9090,\n",
              "         -3.7389, -3.6041, -3.9671, -3.9068, -3.7428, -3.8591, -3.8051, -4.2048,\n",
              "         -3.6326, -4.0238, -3.8546, -3.7166, -3.8654, -3.7567, -4.0507, -3.8353,\n",
              "         -3.5507, -3.8413, -4.0636, -3.8239, -3.7374, -3.8922, -3.7225, -3.8941,\n",
              "         -3.8586, -3.4006, -3.8245, -4.0453, -3.7855, -3.8509, -3.8633, -3.8667,\n",
              "         -3.9995, -3.7476, -3.8277, -3.7430, -3.5913]],\n",
              "       grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb9zyqxcZbis"
      },
      "source": [
        "model = Net().to(device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDcwPgfTeCmX"
      },
      "source": [
        "def encode_text_batch(text_batch, device):\n",
        "    \n",
        "  text_batch_targets_lens = [len(text) for text in text_batch]\n",
        "  text_batch_targets_lens = torch.LongTensor(text_batch_targets_lens)\n",
        "  \n",
        "  text_batch_concat = \"\".join(text_batch)\n",
        "  text_batch_targets = [char2idx[c] for c in text_batch_concat]\n",
        "  text_batch_targets = torch.LongTensor(text_batch_targets)\n",
        "  \n",
        "  return text_batch_targets.to(device), text_batch_targets_lens.to(device)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS0Sur4UZ2wg"
      },
      "source": [
        "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2mDBCdvgroa"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwmtkQFIe-RE"
      },
      "source": [
        "def train(epoch, train_loader = train_loader, device = device, log_interval = log_interval, model = model):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    target, _ = encode_text_batch(text_batch, device)\n",
        "    if (data.size()[0] != 6 or (target.size()[0] >= num_chars or target.size()[0] <= 0)):\n",
        "      continue\n",
        "    loss = loss_func(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({round(100. * batch_idx / len(train_loader), 2)}%)]\\tLoss: {round(loss.item(), 6)}')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67JKkaRGffNu"
      },
      "source": [
        "def test(model = model, device = device, test_loader = test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target\n",
        "      target, _ = encode_text_batch(text_batch, device)\n",
        "      if (data.size()[0] != 6 or (target.size()[0] >= num_chars or target.size()[0] <= 0)):\n",
        "        continue\n",
        "      output = model(data)\n",
        "      test_loss += loss_func(output, target).item()  # sum up batch loss\n",
        "      pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJz6aQjwZjEy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "outputId": "50190cdc-678d-4186-a013-26818ab1830f"
      },
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()\n",
        "  # model.eval()\n",
        "  # test_loss = 0\n",
        "  # correct = 0\n",
        "  # with torch.no_grad():\n",
        "  #     for data, target in test_loader:\n",
        "  #         data, target = data.to(device), target\n",
        "  #         output = model(data)\n",
        "  #         target, _ = encode_text_batch(text_batch, device)\n",
        "  #         test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "  #         pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "  #         correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  # test_loss /= len(test_loader.dataset)\n",
        "\n",
        "  # print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "  #     test_loss, correct, len(test_loader.dataset),\n",
        "  #     100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/7359 (0.0%)]\tLoss: 3.771971\n",
            "Train Epoch: 1 [600/7359 (8.15%)]\tLoss: 3.404247\n",
            "Train Epoch: 1 [1200/7359 (16.3%)]\tLoss: 2.885077\n",
            "Train Epoch: 1 [1800/7359 (24.45%)]\tLoss: 2.811479\n",
            "Train Epoch: 1 [2400/7359 (32.6%)]\tLoss: 2.420431\n",
            "Train Epoch: 1 [3000/7359 (40.75%)]\tLoss: 2.149888\n",
            "Train Epoch: 1 [3600/7359 (48.9%)]\tLoss: 2.074283\n",
            "Train Epoch: 1 [4200/7359 (57.05%)]\tLoss: 1.905657\n",
            "Train Epoch: 1 [4800/7359 (65.2%)]\tLoss: 2.292927\n",
            "Train Epoch: 1 [5400/7359 (73.35%)]\tLoss: 2.137237\n",
            "Train Epoch: 1 [6000/7359 (81.5%)]\tLoss: 1.678244\n",
            "Train Epoch: 1 [6600/7359 (89.65%)]\tLoss: 1.690546\n",
            "Train Epoch: 1 [7200/7359 (97.8%)]\tLoss: 1.755249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-fe777110dda6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;31m# model.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# test_loss = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-6fa2bf3af74f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-7-0568c406bf5c>\", line 14, in __getitem__\n    image = resize_to_fit(image)\n  File \"<ipython-input-6-62757328e678>\", line 6, in resize_to_fit\n    image = imutils.resize(image, width=width)\n  File \"/usr/local/lib/python3.6/dist-packages/imutils/convenience.py\", line 91, in resize\n    resized = cv2.resize(image, dim, interpolation=inter)\ncv2.error: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3723: error: (-215:Assertion failed) inv_scale_x > 0 in function 'resize'\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HoAz4db3Tuz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}